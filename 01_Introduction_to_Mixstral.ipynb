{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d31809e-70a8-4ff6-b658-d12eeedd86d8",
   "metadata": {},
   "source": [
    "# Standard LLMs procedure:\n",
    "- Many LLMs are built on a standard Transformer architecture.\n",
    "- The model usually utilized all the parameters to compute inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332f812-3f43-4b7b-8b0c-f78094fa5c1b",
   "metadata": {},
   "source": [
    "# Introduction to Mixstral 8x7B by Mistral AI\n",
    "- Modifies the standard transformer architecture by using a Sparse Mixture of Experts (SMoE).\n",
    "  - i.e., there are 8 distinct feed-forward neural networks called **experts**.\n",
    "  - at inference time, a **different gating neural network** first chooses to activate 2 of the 8 experts for the next token prediction.\n",
    "- Hence, there is a mix of both performance and better inference speed.\n",
    "    - The model trains on **46.7B** parameters.\n",
    "    - but during inference, it only uses **12.9B** parameters.\n",
    "- So, Mixstral 8x7B model has a performance capabilities of a larger model, but has inference cost/speed of a smaller model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
